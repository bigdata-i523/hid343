
@Misc{www-machinelearningmystery,
  author       = {Jason Brownlee},
  title        = {A gentle introduction to Scikit-Learn: Python Machine Learning Library},
  howpublished = {Online},
  month        = apr,
  year         = {2014},
  owner        = {Jason Brownlee},
  url          = {https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/},
}

@Misc{www-analyticvidhya,
  author       = {Kunal Jain},
  title        = {Scikit-Learn in python - The most important Machine Learnig Tool I learnt last year},
  howpublished = {Online},
  month        = jan,
  year         = {2015},
  owner        = {Kunal Jain},
  url          = {https://www.analyticsvidhya.com/blog/2015/01/scikit-learn-python-machine-learning-tool/},
}

@Misc{www-oreily,
  author       = {Ben Lorica},
  title        = {Six Reasons why I recommend scikit-learn},
  howpublished = {Online},
  month        = oct,
  year         = {2015},
  owner        = {Ben Lorica},
  url          = {https://www.oreilly.com/ideas/six-reasons-why-i-recommend-scikit-learn},
}

@article{tapan-kumar,
Abstract = {This article presents the theoretical derivation as well as practical steps for implementing Naive Bayes (NB) and Logistic Regression (LR) classifiers. A generative learning under Gaussian Naive Bayes assumption and two discriminative learning techniques based on gradient ascent and Newton-Raphson methods are described to estimate the parameters of LR. Some limitation of learning techniques and implementation issues are discussed as well. A set of experiments are performed for both the classifiers under different learning circumstances and their performances are compared. From the experiments, it is observed that LR learning with gradient ascent technique outperforms general NB classifier. However, under Gaussian Naive Bayes assumption, both classifiers NB and LR perform similar.},
Author = {Tapan Kumar, Bhowmik},
ISSN = {1137-3601},
Journal = {Inteligencia Artificial, Vol 18, Iss 56, Pp 14-30 (2015)},
Keywords = {Electronic computers. Computer science},
Number = {56},
volume = {1},
Pages = {14},
Title = {Naive Bayes vs Logistic Regression: Theory, Implementation and Experimental Validation.},
URL = {http://proxyiub.uits.iu.edu/login?url=https://search-ebscohost-com.proxyiub.uits.iu.edu/login.aspx?direct=true&db=edsdoj&AN=edsdoj.0e372b34c5d48bcb72cd437eede1fd1&site=eds-live&scope=site},
Year = {2015},
}

@Misc{www-wikipedia-naivebayes,
  author       = {Wikipedia},
  title        = {Naive Bayes},
  howpublished = {Online},
  month        = nov,
  year         = {2017},
  owner        = {Wikipedia},
  url          = {https://en.wikipedia.org/wiki/Naive_Bayes_classifier},
}

@Article{Sayali,
  author  = {Jadhav D. Sayali, and Channe H. P.},
  title   = {Comparative Study of K-NN, Naive Bayes and Decision Tree Classification Techniques},
  journal = {International Journal of Science and Research (IJSR)},
  year    = {2014},
  volume  = {5},
  number  = {1},
  pages   = {1842-1845},
  month   = jan,
  issn    = {2319-7064},
  url     = {https://www.ijsr.net/archive/v5i1/NOV153131.pdf},
}

@TechReport{Zhang,
  author      = {Zhang H.},
  title       = {The Optimality of Naive Bayes},
  institution = {University of New Brunswick},
  year        = {2004},
  type        = {resreport},
  journal     = {American Association for Artifical Intelligence},
  owner       = {American Association for Artifical Intelligence},
  url         = {http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf},
}

@Misc{www-old.cs,
  author       = {Tehrani A. F., and Cheng W., and Hullermeier E.},
  title        = {Choquistic Regression: Generalizing Logistic Regression Using the Choquet Integral},
  howpublished = {Online},
  month        = jul,
  year         = {2011},
  owner        = {Marburg University},
  url          = {https://www-old.cs.uni-paderborn.de/fileadmin/Informatik/eim-i-is/PDFs/Talk.EUSFLAT.11.pdf},
}

@Misc{www-elitedatascience,
  author       = {EliteDataScience},
  title        = {Modern Machine Learning Algorithms: Strengths and Weaknesses},
  howpublished = {Online},
  month        = may,
  year         = {2016},
  owner        = {EliteDataScience},
  url          = {https://elitedatascience.com/machine-learning-algorithms},
}

@Misc{www-classroom,
  author       = {Nick Robinson},
  title        = {The Disadvantages of Logistic Regression},
  howpublished = {Online},
  year         = {NA},
  owner        = {Nick Robinson},
  url          = {http://classroom.synonym.com/disadvantages-logistic-regression-8574447.html},
}

@Article{fang-logistic,
  author  = {Fang J.},
  title   = {Why Logistic Regression Analyses Are More Reliable Than Multiple Regression Analyses},
  journal = {Journal of Business and Economics},
  year    = {2013},
  volume  = {4},
  number  = {7},
  pages   = {620-633},
  month   = jul,
  issn    = {2155-7590},
  owner   = {Marist College},
  url     = {http://www.academicstar.us/UploadFile/Picture/2014-6/201461494819669.pdf},
}

@Misc{www-washington,
  author       = {Jeff Howbert},
  title        = {Introduction to Machine Learning},
  howpublished = {Online},
  month        = jan,
  year         = {2012},
  owner        = {Washington University},
  url          = {http://courses.washington.edu/css490/2012.Winter/lecture_slides/05b_logistic_regression.pdf},
}

@Article{adarsh,
  author       = {Raj S. A., and Fernando L. J., and Raj S.},
  title        = {Predictive Analytics On Political Data},
  journal      = {World Congress on Computing and Communication Technologies},
  year         = {2017},
  volume       = {10},
  number       = {1109},
  pages        = {93-96},
  issn         = {978-1-5090-5573-9/17},
  howpublished = {Congress},
  language     = {English},
  organization = {World Congress on Computing and Communicaiton Technologies},
}

@article{tweet-logistic,
author={S. T. Indra and L. Wikarsa and R. Turang},
number = {385-389},
volume = {1},
Abstract = {Topics about health, music, sport, and technology are widely discussed in social network sites, especially in Twitter. Sharing information about those topics can enrich one's knowledge as well as increase the awareness of the current trends pertinent to the area of interests. Hence, this research aims to develop a web-based application that can classify tweets of netizens into these four categories of topics using one of machine learning methods called Logistic Regression. There are four main processes applied in this application that are fetching tweets, preprocessing, text feature extraction and machine learning. There are 1800 labeled tweets for each topic used as training data. Several processes were done in the pre-processing phase, including removal of URLs, punctuation, and stop words, tokenization, and stemming. Later, the application automatically converted the pre-processed tweets into set of features vector using Bag of Words. The set of features vector was applied to the L},
ISSN = {978-1-5090-4629-4},
Journal = {2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS), Advanced Computer Science and Information Systems (ICACSIS), 2016 International Conference on},
Keywords = {Communication, Networking and Broadcast Technologies, Components, Circuits, Devices and Systems, Computing and Processing, Robotics and Control Systems, Signal Processing and Analysis, Twitter, Logistics, Feature extraction, Training, Mathematical model, Learning systems, machine learning, topic analysis, health, music, sport, technology, text classification, logistic regression},
Pages = {385},
Title = {Using logistic regression method to classify tweets into the selected topics.},
URL = {http://proxyiub.uits.iu.edu/login?url=https://search-ebscohost-com.proxyiub.uits.iu.edu/login.aspx?direct=true&db=edseee&AN=edseee.7872727&site=eds-live&scope=site},
Year = {2016},
}

@INPROCEEDINGS{knn-chung, 
author={J. Jiaqi and Y. Chung}, 
booktitle={2017 IEEE International Conference on Information and Automation (ICIA)}, 
title={Research on K nearest neighbor join for big data},
address = {Department of Computer Engineering Wonkwang University Iksan 54538, Korean},
year={2017}, 
volume={}, 
number={}, 
publisher = {IEEE},
pages={1077-1081}, 
abstract={K Nearest Neighbor Join (KNN Join) is a primitive operation widely adopted by many data mining applications. As a combination of the k nearest neighbor query and the join operation, KNN Join is a computationally intensive algorithm; however, with the increase of data volume and data dimension, the results can't be obtained within acceptable time when this algorithm runs on a single machine. Consequently, on the basis of Spark, a new approach that employs Locality-Sensitive Hashing (LSH) is proposed. The LSH algorithm first maps similar objects onto the same bucket, which can reduce the set of k nearest neighbors; then the distance of objects in the cluster, can be calculated based on Spark. The experimental results show that this proposed approach is accurate and effective for high dimensional big data.}, 
keywords={Big Data;data mining;query processing;K nearest neighbor join;KNN Join;LSH algorithm;Spark;computationally intensive algorithm;data dimension;data mining applications;data volume;high dimensional big data;k nearest neighbor query;locality-sensitive hashing;Approximation algorithms;Big Data;Clustering algorithms;Data mining;Measurement;Sparks;Virtual machining;Big Data;KNN Join;Spark}, 
doi={10.1109/ICInfA.2017.8079062}, 
ISSN={}, 
month={July},}


@Article{Lee2017-knn,
author="Lee, Jae Moon",
title="Fast k-Nearest Neighbor Searching in Static Objects",
journal="Wireless Personal Communications",
year="2017",
month="Mar",
day="01",
volume="93",
number="1",
pages="147--160",
abstract="The k-nearest neighbor searching is a classical problem that has been seriously studied, due to its many important applications. The paper proposes an efficient algorithm to search the k-nearest neighbors for static objects. Since locations of static objects are known in advance and not changed, most of existing solutions build a kd-tree as a preprocessing and search the k- nearest neighbors by using it. We propose a completely different preprocessing with kd-trees. The core idea of this paper is to build in advance the k-nearest neighbors of each static object as a preprocessing. If a querying point q is given, the nearest object p of q is firstly searched and then the k-nearest neighbors of q are found by using the k-nearest neighbors of p. It is to use the feature that two objects may share many neighbors if they are spatially close to each other. In order to measure the performance of the proposed algorithm, we have a number of experiments. The results of experiments showed that the proposed algorithm is 2--3 times quicker than the method using kd-tree in the Point Cloud Library(PCL).",
issn="1572-834X",
doi="10.1007/s11277-016-3524-1",
url="https://doi.org/10.1007/s11277-016-3524-1"
}

@Misc{www-revoledu,
  author       = {Teknomo K},
  title        = {K-Nearest Neighbor Tutorial},
  howpublished = {Online},
  year         = {2017},
  url          = {http://people.revoledu.com/kardi/tutorial/KNN/Strength%20and%20Weakness.htm},
}

@Misc{www-cs.man,
  author       = {Ray M.},
  title        = {Nearest Neighbours: Pros and Cons},
  howpublished = {Online},
  month        = apr,
  year         = {2012},
  url          = {http://www2.cs.man.ac.uk/~raym8/comp37212/main/node264.html},
}

@Misc{www-nickgillian,
  author       = {Nick Gillian},
  title        = {KNN},
  howpublished = {Online},
  month        = apr,
  year         = {2014},
  url          = {http://www.nickgillian.com/wiki/pmwiki.php/GRT/KNN},
}


@article{ERTUGRUL2017480,
title = "A novel version of k nearest neighbor: Dependent nearest neighbor",
journal = "Applied Soft Computing",
volume = "55",
number = "Supplement C",
pages = "480 - 490",
year = "2017",
issn = "1568-4946",
doi = "https://doi.org/10.1016/j.asoc.2017.02.020",
url = "http://www.sciencedirect.com/science/article/pii/S1568494617300984",
author = "Ãmer Faruk ErtuÄrul and Mehmet Emin TaÄluk",
keywords = "Dependency, Similarity, k nearest neighbor, Dependent nearest neighbor",
abstract = "Abstract k nearest neighbor (kNN) is one of the basic processes behind various machine learning methods In kNN, the relation of a query to a neighboring sample is basically measured by a similarity metric, such as Euclidean distance. This process starts with mapping the training dataset onto a one-dimensional distance space based on the calculated similarities, and then labeling the query in accordance with the most dominant or mean of the labels of the k nearest neighbors, in classification or regression issues, respectively. The number of nearest neighbors (k) is chosen according to the desired limit of success. Nonetheless, two distinct samples may have equal distances to query but, with different angles in the feature space. The similarity of the query to these two samples needs to be weighted in accordance with the angle going between the query and each of the samples to differentiate between the two distances in reference to angular information. This opinion can be analyzed in the context of dependency and can be utilized to increase the precision of classifier. With this point of view, instead of kNN, the query is labeled according to its nearest dependent neighbors that are determined by a joint function, which is built on the similarity and the dependency. This method, therefore, may be called dependent NN (d-NN). To demonstrate d-NN, it is applied to synthetic datasets, which have different statistical distributions, and 4 benchmark datasets, which are Pima Indian, Hepatitis, approximate Sinc and CASP datasets. Results showed the superiority of d-NN in terms of accuracy and computation cost as compared to other employed popular machine learning methods."
}

@Misc{www-simafore,
  author       = {Desphpande B.},
  title        = {4 key advanatages of using decision trees for predictive analytics},
  howpublished = {Online},
  month        = jul,
  year         = {2011},
  url          = {http://www.simafore.com/blog/bid/62333/4-key-advantages-of-using-decision-trees-for-predictive-analytics},
}

@Misc{www-cs.ubb,
  author       = {Petri C.},
  title        = {Decison Trees},
  howpublished = {Online},
  year         = {2010},
  url          = {http://www.cs.ubbcluj.ro/~gabis/DocDiplome/DT/DecisionTrees.pdf},
}

@article{HASSAN201752,
title = "Potential of four different machine-learning algorithms in modeling daily global solar radiation",
journal = "Renewable Energy",
volume = "111",
number = "Supplement C",
pages = "52 - 62",
year = "2017",
issn = "0960-1481",
doi = "https://doi.org/10.1016/j.renene.2017.03.083",
url = "http://www.sciencedirect.com/science/article/pii/S0960148117302744",
author = "Muhammed A. Hassan and A. Khalil and S. Kaseb and M.A. Kassem",
keywords = "Global solar radiation, Neural network, ANFIS, Decision trees, Regression analysis, Support Vector Machines",
abstract = "Abstract In this study, the potential of different machine-learning algorithms in modeling global horizontal solar irradiation is examined. Multi-layer perceptron (MLP), adaptive neuro-fuzzy inference system (ANFIS) and Support Vector Machines (SVM) algorithms are adopted, beside a newly suggested algorithm: decision trees. All models are grouped in four categories: sunshine-, temperature-, meteorological parameters- and day number-based models. All models have been trained, optimized, validated and compared with each other and with old and newly suggested regression models, using high-resolution, highly accurate measured data recorded over Cairo, Egypt, throughout five years, as a case study. Models with best statistical measures of accuracy and best generalization abilities have been recommended after being tested using an independent dataset. The results show that MLP models excel in estimating global irradiation with root mean square error lower than that of best corresponding regression models by 4.75â31.69%, depending on the model category. Followed by ANFIS models (if carefully validated) and SVM models. In addition, the study assesses the ability of decision trees in modeling solar radiation. Despite of their simplicity, the merits of temperature- and day number-based models are demonstrated, with coefficients of determination greater than 85%, to be used in case of unavailability of sunshine records."
}

@Misc{www-cs.princeton,
  author       = {Princeton University},
  year = {NA},
  title        = {Decision Tree Learning},
  howpublished = {Online},
  url          = {http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf},
}

@Unpublished{fan-stochastic,
  author = {Fan M.},
  title  = {How and Why to Use Stochastic Gradient Descent?},
  url    = {http://anson.ucdavis.edu/~minjay/SGD.pdf},
  year   = {n.d.},
}

@Misc{www-bottou,
  author       = {Bottou L.},
  title        = {Stochastic Gradient Descent},
  howpublished = {Online},
  year         = {2010},
  url          = {http://leon.bottou.org/projects/sgd},
}

@Article{Needell2016,
author="Needell, Deanna
and Srebro, Nathan
and Ward, Rachel",
title="Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm",
journal="Mathematical Programming",
year="2016",
month="Jan",
day="01",
volume="155",
number="1",
pages="549--573",
abstract="We obtain an improved finite-sample guarantee on the linear convergence of stochastic gradient descent for smooth and strongly convex objectives, improving from a quadratic dependence on the conditioning                                                                           {\$}{\$}(L/{\backslash}mu )^2{\$}{\$}                                                                                                              (                          L                          /                          $\mu$                          )                                                2                                                                             (where                                                                           {\$}{\$}L{\$}{\$}                                                            L                                                       is a bound on the smoothness and                                                                           {\$}{\$}{\backslash}mu {\$}{\$}                                                            $\mu$                                                       on the strong convexity) to a linear dependence on                                                                           {\$}{\$}L/{\backslash}mu {\$}{\$}                                                                                    L                        /                        $\mu$                                                                            . Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence in the average smoothness, dominating previous results. We also discuss importance sampling for SGD more broadly and show how it can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods. In particular, we recast the randomized Kaczmarz algorithm as an instance of SGD, and apply our results to prove its exponential convergence, but to the solution of a weighted least squares problem rather than the original least squares problem. We then present a modified Kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate.",
issn="1436-4646",
doi="10.1007/s10107-015-0864-7",
url="https://doi.org/10.1007/s10107-015-0864-7"
}

@Misc{www-wikipedia-futurescaling,
  author       = {Wikipedia},
  title        = {Feauture Scaling},
  howpublished = {Online},
  year         = {NA},
  url          = {https://en.wikipedia.org/wiki/Feature_scaling},
}

@Misc{www-scikit-stochastic-gradient,
  author       = {NA},
  title        = {Stochastic Gradient Descent},
  howpublished = {Online},
  year         = {NA},
}

@InProceedings{andrewng-deeplearning,
  author       = {Le V. Q., Ngiam J., Coates A., Lahiri A., Prochnow B., Ng A. Y.},
  address      = {Stanford University},
  title        = {On optimization methods for deep learning},
  booktitle    = {International Conference of Machine Learning},
  year         = {2011},
  pages        = {NA},
  organization = {Stanford University},
  publisher    = {International Conferenfe of Machine Learning},
  owner        = {Stanford University},
  url          = {https://cs.stanford.edu/~acoates/papers/LeNgiCoaLahProNg11.pdf},
}

@Misc{www-simafore-svm,
  author       = {Deshpande B.},
  title        = {When do support vector machines trump other classification methods},
  howpublished = {Online},
  month        = jan,
  year         = {2013},
  url          = {http://www.simafore.com/blog/bid/112816/When-do-support-vector-machines-trump-other-classification-methods},
}

@Booklet{berlin-svm,
  title        = {Support Vector Machines (SVM) as a Technique for Solvency Analysis},
  author       = {Auria L, and Moro A. R.},
  howpublished = {Online},
  month        = aug,
  year         = {2008},
  editor       = {DIW Berlin},
  publisher    = {DIW Berline},
  url          = {http://www.diw.de/english/products/publications/discussion_papers/27539.html},
}

@Article{verma-ssv,
  author  = {Shrivastava K. N., and Saurabh P., and Verma B.},
  title   = {An Efficient Approach Parallel Support Vector Machine for Classification of Diabetes Dataset},
  journal = {International Journal of Computer Applications in Technology},
  year    = {2011},
  volume  = {36},
  number  = {6},
  pages   = {19-24},
  month   = dec,
  issn    = {09758887},
  url     = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.259.3757&rep=rep1&type=pdf},
}

@Article{Burges1998,
author="Burges, Christopher J.C.",
title="A Tutorial on Support Vector Machines for Pattern Recognition",
journal="Data Mining and Knowledge Discovery",
year="1998",
month="Jun",
day="01",
volume="2",
number="2",
pages="121--167",
abstract="The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.",
issn="1573-756X",
doi="10.1023/A:1009715923555",
url="https://doi.org/10.1023/A:1009715923555"
}

@Unpublished{dietterich-ensemble,
  author = {Dietterich G. T.},
  title  = {Ensemble Methods in Machine Learning},
  url    = {http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf},
  year   = {n.d.},
}

@Misc{www-pandas,
  author       = {NA},
  title        = {Pandas for Python},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://pandas.pydata.org/},
}

@Misc{www-commasep,
  author       = {Wikipedia},
  title        = {Comma Seperated Values},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://en.wikipedia.org/wiki/Comma-separated_values},
}

@Misc{www-pandas.describe,
  author       = {NA},
  title        = {Pandas Dateframe describe},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html},
}

@Misc{www-pandas.replace,
  author       = {Python Pandas},
  title        = {Dataframe replace},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html},
}

@Misc{www-onlinestat,
  author       = {Lane M. D.},
  year         = {n.d.},
  title        = {Log Transformations},
  howpublished = {Online},
  url          = {http://onlinestatbook.com/2/transformations/log.html},
}

@Misc{www-hackernoon,
  author       = {Vasudev R.},
  title        = {What is One Hot Encoding? do you have to use it ?},
  howpublished = {Online},
  year         = {n.d.},
  month        = aug,
  url          = {https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f},
}

@Misc{www-uci,
  author       = {Kohavi R. and Becker B.},
  title        = {Predicting whether income exceeds \$50K/yr based on census data},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://archive.ics.uci.edu/ml/datasets/Census+Income},
}

@inproceedings{Kohavi:1996:SUA:3001460.3001502,
 author = {Kohavi, Ron},
 address = {Silicon Graphics, Inc},
 title = {Improving the Accuracy of Naive-Bayes Classifiers: A Decision-tree Hybrid},
 booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
 series = {KDD'96},
 year = {1996},
 location = {Portland, Oregon},
 pages = {202--207},
 url = {http://dl.acm.org/citation.cfm?id=3001460.3001502},
 acmid = {3001502},
 publisher = {AAAI Press},
} 

@Misc{www-wikipedia.decision,
  author       = {Wikipedia},
  title        = {Decision Trees},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://en.wikipedia.org/wiki/Decision_tree},
}

@Misc{www-investopedia,
  author       = {Investopedia},
  title        = {Correlation Coefficient},
  howpublished = {Online},
  year         = {n.d.},
  url          = {https://www.investopedia.com/terms/c/correlationcoefficient.asp},
}

@Misc{www-salford,
  author       = {Steinberg D.},
  title        = {Why Data Scientist Split Data into Train and Test},
  howpublished = {Online},
  month        = mar,
  year         = {2014},
  url          = {https://info.salford-systems.com/blog/bid/337783/Why-Data-Scientists-Split-Data-into-Train-and-Test},
}